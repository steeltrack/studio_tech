{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Query Hybrid Database\n",
    "- Pass to LLM\n",
    "- Add gear classifier\n",
    "- Update Query to Weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using unstructured locally\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "# https://docs.unstructured.io/examplecode/codesamples/apioss/table-extraction-from-pdf\n",
    "# https://docs.unstructured.io/open-source/concepts/document-elements\n",
    "raw_elements = partition_pdf(\n",
    "    \"documents/Sequential Trigon 6.pdf\",\n",
    "    strategy=\"hi_res\",\n",
    "    multipage_sections=True,\n",
    "    extract_images_in_pdf=False,\n",
    "    infer_table_structure=True,\n",
    "    hi_res_model_name=\"yolox\"\n",
    ")\n",
    "\n",
    "elements = []\n",
    "tables = []\n",
    "\n",
    "skip_list = [\"Footer\", \"Header\", \"Table\"]\n",
    "\n",
    "for raw_element in raw_elements:\n",
    "    # Experimenting with not chunking tables\n",
    "    if raw_element.category == \"Table\":\n",
    "        tables.append(raw_element)\n",
    "    # Elements we don't want in our index\n",
    "    elif raw_element.category not in skip_list:\n",
    "        elements.append(raw_element)\n",
    "\n",
    "# First testing with larger chunks since Claude handles 200k tokens.\n",
    "# TODO: play around with these numbers\n",
    "chunks = chunk_by_title(\n",
    "    elements, \n",
    "    max_characters=10000, \n",
    "    combine_text_under_n_chars=500\n",
    ")\n",
    "\n",
    "print(\"\\n\\n ----chunk---- \\n\\n\".join([chunk.text for chunk in chunks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using unstructured API and proprietary model\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "import unstructured_client\n",
    "from unstructured_client.models import operations, shared\n",
    "\n",
    "client = unstructured_client.UnstructuredClient(\n",
    "    api_key_auth=\"S8kpjp1vPjnZh9WyLr7FDZp5yD0PO3\",\n",
    "    server_url=\"https://platform.unstructuredapp.io\"\n",
    ")\n",
    "\n",
    "filename = \"documents/Sequential Trigon 6.pdf\"\n",
    "with open(filename, \"rb\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "req = operations.PartitionRequest(\n",
    "    partition_parameters=shared.PartitionParameters(\n",
    "        files=shared.Files(\n",
    "            content=data,\n",
    "            file_name=filename\n",
    "        ),\n",
    "        strategy=shared.Strategy.HI_RES,\n",
    "        hi_res_model_name=\"layout_v1.1.0\"\n",
    "    )\n",
    ")\n",
    "\n",
    "try:\n",
    "    res = client.general.partition(request=req)\n",
    "    print(res.elements[1])\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import anthropic\n",
    "import time\n",
    "from tqdm.notebook import tqdm  # Progress bar for Jupyter\n",
    "\n",
    "# Configuration\n",
    "input_folder = \"document_test\"\n",
    "output_folder = \"output/extractions\"\n",
    "prompt_template = \"You are a PDF to markdown converter. Convert the following extracted pdf output to markdown. Ignore page headers and page footers. Keep semantic markup like headings, bold, italics, and bullets. Do not respond with anything except the extracted markdown. The input follows: {}\" \n",
    "\n",
    "# Initialize Claude client\n",
    "client = anthropic.Client()\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "Path(output_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Get all PDF files in the input folder\n",
    "pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Processing PDF files\"):\n",
    "    file_path = os.path.join(input_folder, pdf_file)\n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "    \n",
    "    # Prepare variables\n",
    "    filename_base = os.path.splitext(pdf_file)[0]\n",
    "    doc = None\n",
    "    \n",
    "    # Prepare the results container for this PDF\n",
    "    pdf_results = {\n",
    "        'filename': pdf_file,\n",
    "        'path': file_path,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'pages': []\n",
    "    }\n",
    "\n",
    "    markdown_output = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Open the PDF\n",
    "        doc = fitz.open(file_path)\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num in tqdm(range(len(doc)), desc=f\"Pages in {pdf_file}\"):\n",
    "            # For testing since long docs are spendy.\n",
    "            #if page_num > 5:\n",
    "            #   break;\n",
    "            page = doc.load_page(page_num)\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            # Skip empty pages\n",
    "            if not page_text.strip():\n",
    "                print(f\"  Page {page_num+1} is empty, skipping\")\n",
    "                pdf_results['pages'].append({\n",
    "                    'page_number': page_num + 1,\n",
    "                    'status': 'skipped',\n",
    "                    'reason': 'empty page'\n",
    "                })\n",
    "                continue\n",
    "                \n",
    "            # Prepare the prompt with the page content\n",
    "            prompt = prompt_template.format(page_text)\n",
    "            \n",
    "            # Call Claude API\n",
    "            try:\n",
    "                response = client.messages.create(\n",
    "                    model=\"claude-3-7-sonnet-latest\",\n",
    "                    max_tokens=8192,\n",
    "                    system=\"You are an assistant that analyzes PDF content.\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                # Store the response in our results\n",
    "                pdf_results['pages'].append({\n",
    "                    'page_number': page_num + 1,\n",
    "                    'status': 'success',\n",
    "                    'response': response.content[0].text\n",
    "                })\n",
    "\n",
    "                markdown_output += response.content[0].text + \"\\n\\n\"\n",
    "                \n",
    "                # Respect rate limits - add a small delay\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as api_error:\n",
    "                print(f\"  API error on page {page_num+1}: {api_error}\")\n",
    "                # Record the error but continue with next page\n",
    "                pdf_results['pages'].append({\n",
    "                    'page_number': page_num + 1,\n",
    "                    'status': 'error',\n",
    "                    'error_message': str(api_error)\n",
    "                })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "        pdf_results['error'] = str(e)\n",
    "    \n",
    "    finally:\n",
    "        # Close the document if it was successfully opened\n",
    "        if doc is not None:\n",
    "            try:\n",
    "                doc.close()\n",
    "            except Exception as close_error:\n",
    "                print(f\"Warning: Could not close document properly: {close_error}\")\n",
    "        \n",
    "        # Save results for this PDF, even if partial due to errors\n",
    "        output_file = os.path.join(output_folder, f\"{filename_base}.md\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_output)\n",
    "        debug_file = os.path.join(output_folder, f\"{filename_base}_results.json\")\n",
    "        with open(debug_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(pdf_results, f)\n",
    "            \n",
    "        print(f\"Saved results for {pdf_file} to {output_file}\")\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import anthropic\n",
    "import time\n",
    "from tqdm.notebook import tqdm  # Progress bar for Jupyter\n",
    "import base64\n",
    "\n",
    "# Configuration\n",
    "input_folder = \"document_test\"\n",
    "output_folder = \"output/extractions\"\n",
    "# TODO: Improve prompt because still getting header and footer content.\n",
    "prompt = \"You are a PDF to markdown converter. Convert the attached pdf to markdown. Make sure to ignore page headers at the top of each page and page footers at the bottom of each page that might contain page numbers, the document name, or the section title. Keep semantic markup like headings, bold, italics, and bullets. If you encounter pictures or diagrams, describe their purpose. Do not respond with anything except the extracted markdown.\"\n",
    "\n",
    "# Initialize Claude client\n",
    "client = anthropic.Client()\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "Path(output_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Get all PDF files in the input folder\n",
    "pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Processing PDF files\"):\n",
    "    file_path = os.path.join(input_folder, pdf_file)\n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "    \n",
    "    # Prepare variables\n",
    "    filename_base = os.path.splitext(pdf_file)[0]\n",
    "    doc = None\n",
    "    \n",
    "    # Prepare the results container for this PDF\n",
    "    pdf_results = {\n",
    "        'filename': pdf_file,\n",
    "        'path': file_path,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'pages': []\n",
    "    }\n",
    "\n",
    "    markdown_output = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Open the PDF\n",
    "        doc = fitz.open(file_path)\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num in tqdm(range(len(doc)), desc=f\"Pages in {pdf_file}\"):\n",
    "            # For testing since long docs are spendy.\n",
    "            #if page_num > 10:\n",
    "            #   break;\n",
    "            \n",
    "            # Create empty pdf, insert page, and then get binary info\n",
    "            page_pdf = fitz.open()\n",
    "            page_pdf.insert_pdf(doc, from_page=page_num, to_page=page_num)\n",
    "            page_bytes = page_pdf.tobytes()\n",
    "            base64_string = base64.b64encode(page_bytes).decode(\"utf-8\")\n",
    "            \n",
    "            # https://github.com/anthropics/anthropic-cookbook/blob/main/misc/pdf_upload_summarization.ipynb\n",
    "            # Call Claude API\n",
    "            try:\n",
    "                response = client.messages.create(\n",
    "                    model=\"claude-3-7-sonnet-latest\",\n",
    "                    max_tokens=8192,\n",
    "                    system=\"You are an assistant that analyzes PDF content.\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\", \n",
    "                            \"content\": [\n",
    "                                {\"type\": \"document\", \"source\": {\"type\": \"base64\", \"media_type\": \"application/pdf\", \"data\": base64_string}},\n",
    "                                {\"type\": \"text\", \"text\": prompt}\n",
    "                            ]\n",
    "                         }\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                # Store the response in our results\n",
    "                pdf_results['pages'].append({\n",
    "                    'page_number': page_num + 1,\n",
    "                    'status': 'success',\n",
    "                    'response': response.content[0].text\n",
    "                })\n",
    "\n",
    "                markdown_output += response.content[0].text + \"\\n\\n\"\n",
    "                \n",
    "                # Respect rate limits - add a small delay\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as api_error:\n",
    "                print(f\"  API error on page {page_num+1}: {api_error}\")\n",
    "                # Record the error but continue with next page\n",
    "                pdf_results['pages'].append({\n",
    "                    'page_number': page_num + 1,\n",
    "                    'status': 'error',\n",
    "                    'error_message': str(api_error)\n",
    "                })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "        pdf_results['error'] = str(e)\n",
    "    \n",
    "    finally:\n",
    "        # Close the document if it was successfully opened\n",
    "        if doc is not None:\n",
    "            try:\n",
    "                doc.close()\n",
    "            except Exception as close_error:\n",
    "                print(f\"Warning: Could not close document properly: {close_error}\")\n",
    "        \n",
    "        # Save results for this PDF, even if partial due to errors\n",
    "        output_file = os.path.join(output_folder, f\"{filename_base}_uploaded.md\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_output)\n",
    "        debug_file = os.path.join(output_folder, f\"{filename_base}_results_uploaded.json\")\n",
    "        with open(debug_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(pdf_results, f)\n",
    "            \n",
    "        print(f\"Saved results for {pdf_file} to {output_file}\")\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the winner. We take a PDF, and extract it page by page into a new pdf which is then uploaded to the LLM with specific instructions on how to convert it to markdown. It took some prompt refinement to get it to behave correctly, but now it handles headers, footers, tables, and formatting really well. Image interpretation is a little hit and miss, but given we aren't keeping images I think that's ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import anthropic\n",
    "import time\n",
    "from tqdm.notebook import tqdm  # Progress bar for Jupyter\n",
    "import base64\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "input_folder = \"document_test\"\n",
    "output_folder = \"output/extractions\"\n",
    "prompt = \"\"\"Please follow these instructions carefully:\n",
    "\n",
    "1. Analyze the PDF content thoroughly.\n",
    "\n",
    "2. Convert the content to markdown format, following these rules:\n",
    "   - Ignore page headers at the top of each page and page footers at the bottom of each page. These often contain page numbers, document names, or section titles.\n",
    "   - Preserve semantic markup such as headings, bold text, italics, and bullet points.\n",
    "   - If you encounter pictures or diagrams, describe their purpose in markdown instead of including the actual images.\n",
    "   - For multi-column layouts, treat the columns as one continuous page, maintaining the logical flow of the content.\n",
    "   - Do not exclude any sections, summarize them, or truncate for length.\n",
    "\n",
    "3. Before providing the final markdown output, wrap your analysis in a <pdf_analysis> tag to show your thought process and ensure you've addressed all requirements. In your analysis:\n",
    "   - List the main sections or chapters of the PDF content.\n",
    "   - Identify and quote examples of headers and footers you'll be ignoring.\n",
    "   - List and describe any images or diagrams you've found.\n",
    "   - Note any special formatting or semantic markup you've encountered.\n",
    "   - Explain how you'll handle multi-column layouts, if present.\n",
    "   - Double check that you didn't truncate any content.\n",
    "   - Outline your plan for converting the content to markdown.\n",
    "\n",
    "4. After your analysis, provide the converted markdown content in <markdown_output></markdown_output> tags without any additional commentary. Don't forget the closing tag.\n",
    "\n",
    "Example output structure:\n",
    "\n",
    "<pdf_analysis>\n",
    "[Your detailed analysis of the PDF content, including:\n",
    "- List of main sections or chapters\n",
    "- Examples of headers and footers\n",
    "- Description of images or diagrams\n",
    "- Notes on special formatting or semantic markup\n",
    "- Approach for handling multi-column layouts\n",
    "- Check that all required content is present and not truncated.\n",
    "- Conversion plan]\n",
    "</pdf_analysis>\n",
    "\n",
    "<markdown_output>\n",
    "[Your converted markdown content here]\n",
    "</markdown_output>\n",
    "\n",
    "Please proceed with your analysis and conversion of the PDF content.\"\"\"\n",
    "\n",
    "# Initialize Claude client\n",
    "client = anthropic.Client()\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "Path(output_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Get all PDF files in the input folder\n",
    "pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Processing PDF files\"):\n",
    "    file_path = os.path.join(input_folder, pdf_file)\n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "    \n",
    "    # Prepare variables\n",
    "    filename_base = os.path.splitext(pdf_file)[0]\n",
    "    doc = None\n",
    "    \n",
    "    # Prepare the results container for this PDF\n",
    "    pdf_results = {\n",
    "        'filename': pdf_file,\n",
    "        'path': file_path,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'pages': []\n",
    "    }\n",
    "\n",
    "    markdown_output = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Open the PDF\n",
    "        doc = fitz.open(file_path)\n",
    "        \n",
    "        # Process each page\n",
    "        for page_num in tqdm(range(len(doc)), desc=f\"Pages in {pdf_file}\"):\n",
    "            # For testing since long docs are spendy.\n",
    "            #if page_num == 3:\n",
    "            #   continue;\n",
    "            \n",
    "            # Create empty pdf, insert page, and then get binary info\n",
    "            page_pdf = fitz.open()\n",
    "            page_pdf.insert_pdf(doc, from_page=page_num, to_page=page_num)\n",
    "            page_bytes = page_pdf.tobytes()\n",
    "            base64_string = base64.b64encode(page_bytes).decode(\"utf-8\")\n",
    "            \n",
    "            # https://github.com/anthropics/anthropic-cookbook/blob/main/misc/pdf_upload_summarization.ipynb\n",
    "            # Call Claude API\n",
    "            try:\n",
    "                response = client.messages.create(\n",
    "                    model=\"claude-3-7-sonnet-latest\",\n",
    "                    max_tokens=8192,\n",
    "                    system=\"You are an advanced AI assistant specializing in PDF content analysis and conversion. Your task is to convert the provided PDF content into markdown format while adhering to specific guidelines.\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\", \n",
    "                            \"content\": [\n",
    "                                {\"type\": \"document\", \"source\": {\"type\": \"base64\", \"media_type\": \"application/pdf\", \"data\": base64_string}},\n",
    "                                {\"type\": \"text\", \"text\": prompt}\n",
    "                            ]\n",
    "                         }\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                # Store the response in our results\n",
    "                pdf_results['pages'].append({\n",
    "                    'page_number': page_num + 1,\n",
    "                    'status': 'success',\n",
    "                    'response': response.content[0].text\n",
    "                })\n",
    "\n",
    "                pattern = r'<markdown_output>(.*?)</markdown_output>'\n",
    "                match = re.search(pattern, response.content[0].text, re.DOTALL)\n",
    "\n",
    "                markdown_output += match.group(1) + \"\\n\"\n",
    "                \n",
    "                # Respect rate limits - add a small delay\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as api_error:\n",
    "                print(f\"  API error on page {page_num+1}: {api_error}\")\n",
    "                # Record the error but continue with next page\n",
    "                pdf_results['pages'].append({\n",
    "                    'page_number': page_num + 1,\n",
    "                    'status': 'error',\n",
    "                    'error_message': str(api_error)\n",
    "                })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "        pdf_results['error'] = str(e)\n",
    "    \n",
    "    finally:\n",
    "        # Close the document if it was successfully opened\n",
    "        if doc is not None:\n",
    "            try:\n",
    "                doc.close()\n",
    "            except Exception as close_error:\n",
    "                print(f\"Warning: Could not close document properly: {close_error}\")\n",
    "        \n",
    "        # Save results for this PDF, even if partial due to errors\n",
    "        output_file = os.path.join(output_folder, f\"{filename_base}.md\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_output)\n",
    "        debug_file = os.path.join(output_folder, f\"{filename_base}_results.json\")\n",
    "        with open(debug_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(pdf_results, f)\n",
    "            \n",
    "        print(f\"Saved results for {pdf_file} to {output_file}\")\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm  # Progress bar for Jupyter\n",
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "import anthropic\n",
    "\n",
    "# Initialize Claude client\n",
    "client = anthropic.Client()\n",
    "\n",
    "def situate_context(doc: str, chunk: str):\n",
    "    DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "    <document>\n",
    "    {doc_content}\n",
    "    </document>\n",
    "    \"\"\"\n",
    "\n",
    "    CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "    Here is the chunk we want to situate within the whole document\n",
    "    <chunk>\n",
    "    {chunk_content}\n",
    "    </chunk>\n",
    "\n",
    "    Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "    Answer only with the succinct context and nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"} # Cache full document context\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def classify_content(doc: str):\n",
    "    CLASSIFIER_PROMPT = \"\"\"\n",
    "    You will be analyzing a technical manual for a product to extract specific information. The manual content is provided below:\n",
    "\n",
    "    <technical_manual>\n",
    "    {doc_content}\n",
    "    </technical_manual>\n",
    "\n",
    "    Your task is to carefully read through the manual and extract the following information:\n",
    "    1. Company name / brand\n",
    "    2. Model name of the product being documented\n",
    "    3. Type of product (e.g., synthesizer, guitar pedal, software plugin)\n",
    "    4. Keywords that describe the purpose and utility of the product (to aid in BM25 search)\n",
    "\n",
    "    Follow these steps to complete the task:\n",
    "\n",
    "    1. Thoroughly read the entire technical manual.\n",
    "\n",
    "    2. Look for the company name or brand. This is often found on the cover page, in headers, or in copyright notices.\n",
    "\n",
    "    3. Identify the model name of the product. This is typically prominently displayed near the beginning of the manual or in product descriptions.\n",
    "\n",
    "    4. Determine the type of product based on the descriptions and features mentioned in the manual.\n",
    "\n",
    "    5. Extract keywords that describe the product's purpose and utility. Focus on terms that highlight its main features, functions, and applications.\n",
    "\n",
    "    6. Organize your findings into a JSON object with the following structure:\n",
    "    {{\n",
    "        \"company_name\": \"\",\n",
    "        \"model_name\": \"\",\n",
    "        \"product_type\": \"\",\n",
    "        \"keywords\": []\n",
    "    }}\n",
    "\n",
    "    Important notes:\n",
    "    - If you cannot find a specific piece of information, use \"Unknown\" as the value.\n",
    "    - For the \"keywords\" field, include an array of relevant terms (at least 3, but no more than 10).\n",
    "    - Ensure that the extracted information is accurate and directly supported by the content in the manual.\n",
    "    - Leave out legal designations like LLC or TM.\n",
    "\n",
    "    Present your final output within <json_output> tags, formatted as a valid JSON object.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": CLASSIFIER_PROMPT.format(doc_content=doc),\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"} # Cache full document context\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Pattern to match content between <json_output> tags\n",
    "    pattern = r'<json_output>(.*?)</json_output>'\n",
    "    \n",
    "    # Use re.DOTALL to make '.' match newlines as well\n",
    "    match = re.search(pattern, response.content[0].text, re.DOTALL)\n",
    "\n",
    "    empty_object =  {\n",
    "        \"company_name\": \"\",\n",
    "        \"model_name\": \"\",\n",
    "        \"product_type\": \"\",\n",
    "        \"keywords\": []\n",
    "    }\n",
    "\n",
    "    if match:\n",
    "        try:\n",
    "            # Extract the JSON string and parse it\n",
    "            json_str = match.group(1).strip()\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "            return empty_object\n",
    "    else:\n",
    "        return empty_object\n",
    "\n",
    "# Configuration\n",
    "input_folder = \"output/extractions\"\n",
    "output_folder = \"output/chunks\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "Path(output_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Get all Markdown files in the input folder\n",
    "md_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.md')]\n",
    "\n",
    "# Process each Markdown file\n",
    "for md_file in md_files:\n",
    "    print(f\"Processing: {md_file}\")\n",
    "    file_path = os.path.join(input_folder, md_file)\n",
    "    filename_base = os.path.splitext(md_file)[0]\n",
    "    output_folder_chunk_path = output_folder + '/' + filename_base\n",
    "    \n",
    "    # Create chunk folder if it doesn't exist\n",
    "    Path(output_folder_chunk_path).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    document = open(file_path, \"r\").read()\n",
    "    elements = partition_md(filename=file_path)\n",
    "\n",
    "    # Not chunking tables, so pulling them out\n",
    "    tables = []\n",
    "    for element in elements:\n",
    "        if element.category == \"Table\":\n",
    "            tables.append(element)\n",
    "        \n",
    "    for table in tables:\n",
    "        # Create metadata\n",
    "        chunk_data = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"source_file\": md_file,\n",
    "            \"category\": \"Table\",\n",
    "            \"content\": table.text,\n",
    "            \"contextualization\": situate_context(document, table.text),\n",
    "            \"raw_table\": table.metadata.text_as_html,\n",
    "            \"created_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        # Generate unique filename using UUID\n",
    "        filename = f\"{chunk_data['id']}.json\"\n",
    "        output_path = os.path.join(output_folder_chunk_path, filename)\n",
    "\n",
    "        # Save chunk as JSON file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunk_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # \"For technical manuals, I recommend larger chunk sizes around 300-500 tokens with semantic boundaries.\"\n",
    "    # \"Use 10% overlap to preserve cross-references.\"\n",
    "    chunks = chunk_by_title(\n",
    "        elements,\n",
    "        multipage_sections=True,\n",
    "        combine_text_under_n_chars=1200,\n",
    "        max_characters=2000,\n",
    "        overlap=60\n",
    "    )\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if chunk.category in [\"Table\", \"TableChunk\"]:\n",
    "            continue\n",
    "        # Create metadata\n",
    "        chunk_data = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"source_file\": md_file,\n",
    "            \"category\": chunk.category,\n",
    "            \"content\": chunk.text,\n",
    "            \"contextualization\": situate_context(document, chunk.text),\n",
    "            \"created_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        # Generate unique filename using UUID\n",
    "        filename = f\"{chunk_data['id']}.json\"\n",
    "        output_path = os.path.join(output_folder_chunk_path, filename)\n",
    "\n",
    "        # Save chunk as JSON file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunk_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Create metadata.json file to classify the pdf as a whole\n",
    "    doc_metadata = classify_content(document)\n",
    "    metadata_filename = f\"metadata.json\"\n",
    "    metadata_output_path = os.path.join(output_folder_chunk_path, metadata_filename)\n",
    "\n",
    "    # Save metadata\n",
    "    with open(metadata_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(doc_metadata, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    print(f\"Completed processing {md_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai\n",
    "import os\n",
    "import json\n",
    "\n",
    "embedding_client = voyageai.Client()\n",
    "\n",
    "root_folder = \"output/chunks\"  # Replace with your actual path\n",
    "\n",
    "for subdir_name in os.listdir(root_folder):\n",
    "    subdir_path = os.path.join(root_folder, subdir_name)\n",
    "    \n",
    "    # Check if this is a subfolder (not the root)\n",
    "    if os.path.isdir(subdir_path):\n",
    "\n",
    "        # Create output folder if it doesn't exist\n",
    "        output_folder = os.path.join(\"output/embeddings\", subdir_name)\n",
    "        Path(output_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Look for JSON files in the current subfolder\n",
    "        json_files = [f for f in os.listdir(subdir_path) if f.lower().endswith('.json')]\n",
    "\n",
    "        # Process each JSON file found\n",
    "        for json_file in json_files:\n",
    "\n",
    "            file_path = os.path.join(subdir_path, json_file)\n",
    "            print(f\"Processing: {file_path}\")\n",
    "\n",
    "            if json_file == \"metadata.json\":\n",
    "                filename = f\"metadata.json\"\n",
    "                output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "                # Save chunk as JSON file\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Read and parse the JSON file\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                    documents = [data['content'] + \"\\n\\n\" + data['contextualization']]\n",
    "\n",
    "                    embeddings = embedding_client.embed(\n",
    "                        documents,\n",
    "                        model=\"voyage-3\",\n",
    "                        input_type=\"document\"\n",
    "                    ).embeddings[0]\n",
    "\n",
    "                    data['embeddings'] = embeddings\n",
    "\n",
    "                    filename = f\"{data['id']}.json\"\n",
    "                    output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "                    # Save chunk as JSON file\n",
    "                    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "print(\"Embedding generation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "print(client.is_ready())  # Should print: `True`\n",
    "\n",
    "root_folder = \"output/embeddings\"  # Replace with your actual path\n",
    "\n",
    "collection_name = \"Manuals\"\n",
    "\n",
    "if client.collections.exists(collection_name):\n",
    "    client.collections.delete(collection_name)\n",
    "\n",
    "manuals = client.collections.create(\n",
    "    \"Manuals\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.none()\n",
    ")\n",
    "\n",
    "for subdir_name in os.listdir(root_folder):\n",
    "    subdir_path = os.path.join(root_folder, subdir_name)\n",
    "    \n",
    "    # Check if this is a subfolder (not the root)\n",
    "    if os.path.isdir(subdir_path):\n",
    "\n",
    "        metadata_file = os.path.join(subdir_path, 'metadata.json')\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Look for JSON files in the current subfolder\n",
    "        json_files = [f for f in os.listdir(subdir_path) if f.lower().endswith('.json')]\n",
    "\n",
    "        # Process each JSON file found\n",
    "        for json_file in json_files:\n",
    "\n",
    "            if json_file == \"metadata.json\":\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(subdir_path, json_file)\n",
    "\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            uuid = manuals.data.insert(\n",
    "                uuid=data[\"id\"],\n",
    "                vector=data[\"embeddings\"],\n",
    "                properties={\n",
    "                    \"content\": data[\"content\"],\n",
    "                    \"doc_type\": \"chunk\",\n",
    "                    \"company_name\": metadata[\"company_name\"].lower(),\n",
    "                    \"model_name\": metadata[\"model_name\"].lower(),\n",
    "                    \"product_type\": metadata[\"product_type\"].lower(),\n",
    "                    \"keywords\": \",\".join(metadata[\"keywords\"]).lower()\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(uuid)\n",
    "\n",
    "client.close()  # Free up resources\n",
    "\n",
    "print (\"Completed load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete existing weaviate collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "print(client.is_ready())  # Should print: `True`\n",
    "\n",
    "root_folder = \"output/embeddings\"  # Replace with your actual path\n",
    "\n",
    "collection_name = \"Manuals\"\n",
    "\n",
    "if client.collections.exists(collection_name):\n",
    "    client.collections.delete(collection_name)\n",
    "\n",
    "manuals = client.collections.create(\n",
    "    \"Manuals\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.none()\n",
    ")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import voyageai\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "embedding_client = voyageai.Client()\n",
    "\n",
    "print(weaviate_client.is_ready())  # Should print: `True`\n",
    "\n",
    "query = \"What waveforms are available on the Trigon 6?\"\n",
    "\n",
    "query_embeddings = embedding_client.embed(\n",
    "    [query],\n",
    "    model=\"voyage-3\",\n",
    "    input_type=\"query\"\n",
    ").embeddings[0]\n",
    "\n",
    "collection = weaviate_client.collections.get(\"Manuals\")\n",
    "\n",
    "response = collection.query.near_vector(\n",
    "    near_vector=query_embeddings,\n",
    "    limit=5,\n",
    "    return_metadata=wvc.query.MetadataQuery(certainty=True)\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "weaviate_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "QueryReturn(objects=[Object(uuid=_WeaviateUUIDInt('e7d89ba1-7149-4559-a223-d5ca51c52467'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'model': 'trigon-6', 'product_type': 'synthesizer', 'doc_type': 'chunk', 'content': 'Oscillator Parameters\\n\\nOctave: Sets the base oscillator frequency of an oscillator over a 5-octave range from -2 to +2, while Oscillator 3 can be dropped to LFO frequency by setting this parameter to lo.\\n\\nThe global Master Tune settings affect the pitch of all oscillators. See \"Globals - Top Row\" on page 13 for more information.\\n\\nPitch: Fine tune control with a range of 7 semitones (a major 5th) up or down. The 12 o\\'clock position is centered. Steps are in cents (50 cents = 1/2 semitone). This can be used to set Oscillators 2 & 3 to different intervals from each other and from Oscillator 1.\\n\\nWaveshape: Triangle, Sawtooth, (Reverse Sawtooth), Pulse—Used to select the waveshape generated by the oscillator. Multiple waveshapes can be set for each oscillator by pressing multiple Waveshapes buttons. This can allow for interesting harmonic-up effects by selecting both sawtooth and pulse, for example.\\n\\nIn the case of Oscillator 3 in lo mode, complex LFO waveforms can be created by selecting multiple Waveshapes. Note also that Oscillator 3 features an additional reverse sawtooth waveshape.\\n\\nPulse Width: Changes the width of the pulse wave from a square wave when the pulse width knob is at center position, to a zero duty cycle pulse (off) at counter clockwise, and a narrow pulse when the pulse width knob is fully clockwise.\\n\\n💡 Applying pulse width modulation using POLY MOD or the LOW FREQUENCY OSCILLATOR is a great way to add movement and thickness to a sound, especially when creating pad or string-like sounds.\\n\\nSync Osc 2: Off, On—Turns Oscillator 2 hard sync on. Sync forces Oscillator 2 to restart its cycle every time Oscillator 1 starts a cycle. This provides a way to create more complex, harmonically rich shapes from simple waveforms—especially when the frequency of Oscillator 2 is set to a different interval than Oscillator 1.', 'brand': 'sequential', 'keywords': 'analog,polyphonic,voltage-controlled,oscillators,filters,amplifiers,performance,sound-shaping,arpeggiator,sequencer'}, references=None, vector={}, collection='Manuals'), Object(uuid=_WeaviateUUIDInt('e7b6745c-130e-45ad-bd30-edfa04c1f3a0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'model': 'trigon-6', 'product_type': 'synthesizer', 'doc_type': 'chunk', 'content': '[Diagram showing three waveforms: 1. Oscillator 1 - sawtooth wave pattern 2. Oscillator 2 - slower sawtooth wave pattern 3. Oscillator 1 synced to Oscillator 2 - modified sawtooth pattern showing sync behavior]\\n\\n💡 Use Poly Mod to sweep the pitch of Oscillator 2 when it is synced to generate the classic, hard-edged sync sound.\\n\\nKeyboard: Off, On—When off, Oscillator 3 ignores the keyboard and note data received via MIDI and plays at its base frequency setting. Oscillator 3 pitch can still be affected by modulation from other sources when in this mode.\\n\\nVolume\\n\\nEach oscillator features a volume control. This allows you to mix the levels of the three oscillators into the filter.\\n\\n[Note: Document icon appears here] Rather than limit the Trigon-6\\'s outputs to keep the instrument from clipping, we allow you to adjust levels at various points in its signal path. This gives you the option to \"overload\" things in interesting ways, if you wish to do so. If not, try reducing the levels of the oscillators using the VOLUME controls. VOLUME at 50% results in relatively clean signals into the filter.\\n\\nFeedback↔Drive\\n\\nThe FDBK ↔ DRIVE knob is bidirectional and controls two separate parameters.\\n\\nWhen the knob is turned clockwise, DRIVE affects the amount of filter saturation by increasing the oscillators\\' overall output.\\n\\nWhen turned counter-clockwise, FEEDBACK can introduce a range of tones from harmonic to chaotic by feeding the Trigon-6\\'s voice card outputs back into the filter stage. This allows a feedback path for each voice.', 'brand': 'sequential', 'keywords': 'analog,polyphonic,voltage-controlled,oscillators,filters,amplifiers,performance,sound-shaping,arpeggiator,sequencer'}, references=None, vector={}, collection='Manuals'), Object(uuid=_WeaviateUUIDInt('db81b9ae-f694-4c43-a9b1-21fe7418a37f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'model': \"mono lancet '15\", 'product_type': 'synthesizer', 'doc_type': 'chunk', 'content': \"Parameter Specification Waveforms Rectangle, Sawtooth, Triangle Octaves 8', 16', 32'\", 'brand': 'vermona', 'keywords': 'analog,voltage-controlled,oscillators,filter,amplifier,modulation,midi,eurorack,modular,performance'}, references=None, vector={}, collection='Manuals'), Object(uuid=_WeaviateUUIDInt('41b65cde-c527-4c10-9acd-7ae5b4719c8c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'model': \"mono lancet '15\", 'product_type': 'synthesizer', 'doc_type': 'chunk', 'content': \"Parameter Specification Waveforms Rectangle, Sawtooth, Noisegenerator Octaves 4', 8', 16' Detune ± 7 seminotes\", 'brand': 'vermona', 'keywords': 'analog,voltage-controlled,oscillators,filter,amplifier,modulation,midi,eurorack,modular,performance'}, references=None, vector={}, collection='Manuals'), Object(uuid=_WeaviateUUIDInt('0873cc10-236a-4753-8db5-493cd03252f6'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'model': 'trigon-6', 'product_type': 'synthesizer', 'doc_type': 'chunk', 'content': \"Oscillator Controls Layout\\n\\nThe synthesizer features three oscillator sections (1, 2, and 3), each with dedicated controls: - Octave selection knobs (-2 to +2 range) - Waveform selection buttons (triangle, sawtooth, pulse) - Pulse width controls - Volume controls - Additional features: - Oscillator 1: Sync OSC 2 and FBDK + DRIV controls - Oscillator 2: Pitch control and Noise level - Oscillator 3: Pitch control and Keyboard tracking\\n\\nOscillators\\n\\nOscillator 2 can be hard-synced to Oscillator 1 for complex, harmonically-rich sounds when modulated.\\n\\nOscillators 2 and 3 feature a fine pitch knob for detuning and thickening sounds. Oscillator 3's octave control features a lo setting that allows it to function as an LFO for modulation purposes, and a keyboard switch that disables keyboard control over its pitch (useful when used as an LFO, or for drones and other effects).\", 'brand': 'sequential', 'keywords': 'analog,polyphonic,voltage-controlled,oscillators,filters,amplifiers,performance,sound-shaping,arpeggiator,sequencer'}, references=None, vector={}, collection='Manuals')])\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import voyageai\n",
    "from weaviate.classes.query import HybridFusion\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "embedding_client = voyageai.Client()\n",
    "\n",
    "print(weaviate_client.is_ready())  # Should print: `True`\n",
    "\n",
    "query = \"What is a sawtooth oscillator?\"\n",
    "\n",
    "query_embeddings = embedding_client.embed(\n",
    "    [query],\n",
    "    model=\"voyage-3\",\n",
    "    input_type=\"query\"\n",
    ").embeddings[0]\n",
    "\n",
    "collection = weaviate_client.collections.get(\"Manuals\")\n",
    "\n",
    "response = collection.query.hybrid(\n",
    "    query=query,\n",
    "    vector=query_embeddings,\n",
    "    limit=5,\n",
    "    fusion_type=HybridFusion.RELATIVE_SCORE,\n",
    "    return_metadata=wvc.query.MetadataQuery(certainty=True)\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "weaviate_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import voyageai\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "embedding_client = voyageai.Client()\n",
    "\n",
    "print(weaviate_client.is_ready())  # Should print: `True`\n",
    "\n",
    "query = \"What waveforms are available on the Trigon 6?\"\n",
    "\n",
    "query_embeddings = embedding_client.embed(\n",
    "    [query],\n",
    "    model=\"voyage-3\",\n",
    "    input_type=\"query\"\n",
    ").embeddings[0]\n",
    "\n",
    "collection = weaviate_client.collections.get(\"Manuals\")\n",
    "\n",
    "response = collection.query.hybrid(\n",
    "    query=query,\n",
    "    vector=query_embeddings,\n",
    "    limit=5,\n",
    "    return_metadata=wvc.query.MetadataQuery(certainty=True)\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for object in response.objects:\n",
    "    documents.append(object.properties['content'])\n",
    "\n",
    "reranked_documents = embedding_client.rerank(\n",
    "    query=query,\n",
    "    documents=documents,\n",
    "    model=\"rerank-2\"\n",
    ")\n",
    "\n",
    "print(reranked_documents.results)\n",
    "\n",
    "weaviate_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import weaviate\n",
    "\n",
    "import weaviate\n",
    "import voyageai\n",
    "from weaviate.classes.query import HybridFusion\n",
    "import weaviate.classes as wvc\n",
    "\n",
    "anthropic_client = anthropic.Client()\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "embedding_client = voyageai.Client()\n",
    "\n",
    "query = \"How do I tie notes together with the Trigon 6's sequencer?\"\n",
    "\n",
    "query_embeddings = embedding_client.embed(\n",
    "    [query],\n",
    "    model=\"voyage-3\",\n",
    "    input_type=\"query\"\n",
    ").embeddings[0]\n",
    "\n",
    "collection = weaviate_client.collections.get(\"Manuals\")\n",
    "\n",
    "response = collection.query.hybrid(\n",
    "    query=query,\n",
    "    vector=query_embeddings,\n",
    "    limit=5,\n",
    "    fusion_type=HybridFusion.RELATIVE_SCORE,\n",
    "    return_metadata=wvc.query.MetadataQuery(certainty=True)\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for object in response.objects:\n",
    "    documents.append(object.properties['content'])\n",
    "\n",
    "system_instructions = \"You are a specialized studio assistant for a busy music producer. Your primary purpose is to provide accurate, concise technical information to maximize the producer's efficiency in the studio. You have access to a RAG (Retrieval-Augmented Generation) system containing a comprehensive index of technical manuals for all studio equipment.\"\n",
    "\n",
    "RAG_PROMPT = \"\"\"\n",
    "Core Responsibilities:\n",
    "\n",
    "1. Answer technical questions about studio equipment using the retrieved manual excerpts\n",
    "2. Provide troubleshooting assistance based on technical documentation\n",
    "3. Suggest optimal equipment settings and configurations\n",
    "4. Offer workflow tips to improve productivity\n",
    "5. Translate technical jargon into clear, actionable instructions\n",
    "\n",
    "Interaction Guidelines:\n",
    "\n",
    "- Keep responses brief and focused on the immediate need\n",
    "- Prioritize actionable information over theoretical explanations\n",
    "- Acknowledge when information is incomplete or unclear in the retrieved documents\n",
    "- Use appropriate technical terminology but explain it when necessary\n",
    "- Format responses for quick scanning (concise paragraphs, occasional bullet points)\n",
    "- Include exact page/section references from manuals when relevant\n",
    "- When suggesting alternatives, focus only on what's feasible with the existing equipment\n",
    "\n",
    "Response Structure:\n",
    "\n",
    "- Direct answer to the question (1-2 sentences)\n",
    "- Supporting details from relevant manual(s)\n",
    "- Practical next steps or troubleshooting sequence (when applicable)\n",
    "- Optional: Quick tip for improved workflow\n",
    "\n",
    "Input Template\n",
    "\n",
    "The following template contains:\n",
    "- <documents> tag containing context from RAG system\n",
    "- <question> tag containing the query to be answered\n",
    "\n",
    "<documents>\n",
    "    {documents}\n",
    "</documents>\n",
    "\n",
    "<question>\n",
    "    {question}\n",
    "</question>\n",
    "\"\"\"\n",
    "\n",
    "llm_response = client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    system=system_instructions,\n",
    "    max_tokens=8192,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": RAG_PROMPT.format(documents=\"\\n\".join(documents), question=query)                    \n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(llm_response.content[0].text)\n",
    "\n",
    "weaviate_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify models and brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.query import Metrics\n",
    "import anthropic\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "collection = weaviate_client.collections.get(\"Manuals\")\n",
    "\n",
    "brand_response = collection.aggregate.over_all(\n",
    "    return_metrics=Metrics(\"company_name\").text(\n",
    "        top_occurrences_count=True,\n",
    "        top_occurrences_value=True,\n",
    "        min_occurrences=5\n",
    "    )\n",
    ")\n",
    "\n",
    "brands = []\n",
    "for occurence in brand_response.properties['company_name'].top_occurrences:\n",
    "    brands.append(occurence.value)\n",
    "\n",
    "model_response = collection.aggregate.over_all(\n",
    "    return_metrics=Metrics(\"model_name\").text(\n",
    "        top_occurrences_count=True,\n",
    "        top_occurrences_value=True,\n",
    "        min_occurrences=5\n",
    "    )\n",
    ")\n",
    "\n",
    "models = []\n",
    "for occurence in model_response.properties['model_name'].top_occurrences:\n",
    "    models.append(occurence.value)\n",
    "\n",
    "print(brands)\n",
    "print(models)\n",
    "\n",
    "anthropic_client = anthropic.Client()\n",
    "\n",
    "MODEL_CLASSIFIER_PROMPT = \"\"\"\n",
    "You will be given a list of brands and models, followed by a user's query. Your task is to determine if the user's query contains mentions of any of the brands or models from the list. Exact matches are not necessary; you should look for close matches or variations as well.\n",
    "\n",
    "First, here is the list of brands and models:\n",
    "<brands>\n",
    "{BRANDS}\n",
    "</brands>\n",
    "\n",
    "<models>\n",
    "{MODELS}\n",
    "</models>\n",
    "\n",
    "Now, here is the user's query:\n",
    "<user_query>\n",
    "{USER_QUERY}\n",
    "</user_query>\n",
    "\n",
    "Analyze the user's query and compare it to the brands and models list. Look for exact matches, close matches, or variations of the brands and models. Consider common misspellings, abbreviations, or partial matches.\n",
    "\n",
    "Provide your response in the following format:\n",
    "\n",
    "<analysis>\n",
    "1. First, list any brands or models you've identified in the user's query. For each match, briefly explain why you consider it a match (e.g., exact match, close spelling, common abbreviation).\n",
    "\n",
    "2. If you haven't found any matches, state that no matches were found.\n",
    "</analysis>\n",
    "\n",
    "<brands>\n",
    "List the matched brands here, one per line. If no matches were found, write \"No matches found.\"\n",
    "</brands>\n",
    "\n",
    "<models>\n",
    "List the matched models here, one per line. If no matches were found, write \"No matches found.\"\n",
    "</models>\n",
    "\n",
    "Remember, your goal is to identify mentions of brands and models from the provided list in the user's query, even if they're not exact matches. Be thorough in your analysis, but avoid false positives by ensuring there's a reasonable connection between the query and the brands/models list.\n",
    "\"\"\"\n",
    "\n",
    "query = \"What oscillators are available on the sequential trigon?\"\n",
    "\n",
    "llm_response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    system=system_instructions,\n",
    "    max_tokens=8192,\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": MODEL_CLASSIFIER_PROMPT.format(BRANDS=\"\\n\".join(brands), MODELS=\"\\n\".join(models), USER_QUERY=query)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(llm_response.content[0].text)\n",
    "\n",
    "weaviate_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.query import Metrics\n",
    "import anthropic\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "collection = weaviate_client.collections.get(\"Manuals\")\n",
    "\n",
    "brand_response = collection.aggregate.over_all(\n",
    "    return_metrics=Metrics(\"company_name\").text(\n",
    "        top_occurrences_count=True,\n",
    "        top_occurrences_value=True,\n",
    "        min_occurrences=5\n",
    "    )\n",
    ")\n",
    "\n",
    "brands = []\n",
    "for occurence in brand_response.properties['company_name'].top_occurrences:\n",
    "    brands.append(occurence.value)\n",
    "\n",
    "model_response = collection.aggregate.over_all(\n",
    "    return_metrics=Metrics(\"model_name\").text(\n",
    "        top_occurrences_count=True,\n",
    "        top_occurrences_value=True,\n",
    "        min_occurrences=5\n",
    "    )\n",
    ")\n",
    "\n",
    "models = []\n",
    "for occurence in model_response.properties['model_name'].top_occurrences:\n",
    "    models.append(occurence.value)\n",
    "\n",
    "print(brands)\n",
    "print(models)\n",
    "\n",
    "anthropic_client = anthropic.Client()\n",
    "\n",
    "MODEL_CLASSIFIER_PROMPT = \"\"\"\n",
    "You will be given a list of brands and models, followed by a user's query. Your task is to determine if the user's query contains mentions of any of the brands or models from the list. Exact matches are not necessary; you should look for close matches or variations as well. Consider common misspellings, abbreviations, or partial matches.\n",
    "\n",
    "First, here is the list of brands and models:\n",
    "<brands>\n",
    "{BRANDS}\n",
    "</brands>\n",
    "\n",
    "<models>\n",
    "{MODELS}\n",
    "</models>\n",
    "\n",
    "Now, here is the user's query:\n",
    "<user_query>\n",
    "{USER_QUERY}\n",
    "</user_query>\n",
    "\n",
    "Provide your response in the following format:\n",
    "\n",
    "<brands>\n",
    "List the matched brands here, one per line. If no matches were found, write \"none\"\n",
    "</brands>\n",
    "\n",
    "<models>\n",
    "List the matched models here, one per line. If no matches were found, write \"none\"\n",
    "</models>\n",
    "\"\"\"\n",
    "\n",
    "query = \"What oscillators are available on the trigon?\"\n",
    "\n",
    "llm_response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": MODEL_CLASSIFIER_PROMPT.format(BRANDS=\"\\n\".join(brands), MODELS=\"\\n\".join(models), USER_QUERY=query)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(llm_response.content[0].text)\n",
    "\n",
    "weaviate_client.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
